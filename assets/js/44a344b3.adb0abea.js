"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[299],{9354:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>n,metadata:()=>i,toc:()=>x});var d=r(5893),s=r(1151);const n={title:"Lectures"},c="Course Schedule",i={type:"mdx",permalink:"/lectures",source:"@site/src/pages/lectures.md",title:"Lectures",description:"|Date|Lecture|Logistics|",frontMatter:{title:"Lectures"},unlisted:!1},l={},x=[];function j(e){const t={h1:"h1",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.a)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(t.h1,{id:"course-schedule",children:"Course Schedule"}),"\n",(0,d.jsxs)(t.table,{children:[(0,d.jsx)(t.thead,{children:(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.th,{children:"Date"}),(0,d.jsx)(t.th,{children:"Lecture"}),(0,d.jsx)(t.th,{children:"Logistics"})]})}),(0,d.jsxs)(t.tbody,{children:[(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"1/15"}),(0,d.jsx)(t.td,{children:"Holiday (MLK Jr. Day)"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"1/17"}),(0,d.jsx)(t.td,{children:"Lecture 1: Course Introduction"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"1/22"}),(0,d.jsx)(t.td,{children:"Lecture 2: ML/DL in Robotics: Overview"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"1/24"}),(0,d.jsx)(t.td,{children:"Lecture 3: ML/DL Refresher Part 1"}),(0,d.jsx)(t.td,{children:"HW1 Out"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"1/29"}),(0,d.jsx)(t.td,{children:"Lecture 4: ML/DL Refresher Part 2"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"1/31"}),(0,d.jsx)(t.td,{children:"Lecture 5: MDP Basics: Imitation Learning Part 1"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/05"}),(0,d.jsx)(t.td,{children:"Lecture 6: Imitation Learning Part 2"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/07"}),(0,d.jsx)(t.td,{children:"Lecture 7: RL Basics: Value/Policy Iteration"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/12"}),(0,d.jsx)(t.td,{children:"Lecture 8: Q-Learning"}),(0,d.jsx)(t.td,{children:"HW1 Due; HW2 Out"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/14"}),(0,d.jsx)(t.td,{children:"Lecture 9: Policy Gradient"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/19"}),(0,d.jsx)(t.td,{children:"Lecture 10: Actor-Critic Algorithms"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/21"}),(0,d.jsx)(t.td,{children:"Lecture 11: Advanced RL Algorithms"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/26"}),(0,d.jsx)(t.td,{children:"Lecture 12: Bandit, Preference-Based Learning"}),(0,d.jsx)(t.td,{children:"HW2 Due; HW3 Out"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"2/28"}),(0,d.jsx)(t.td,{children:"Lecture 13: Exploration"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/04"}),(0,d.jsx)(t.td,{children:"Spring Break"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/06"}),(0,d.jsx)(t.td,{children:"Spring Break"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/11"}),(0,d.jsx)(t.td,{children:"Lecture 14: Model-Based Control Basics"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/13"}),(0,d.jsx)(t.td,{children:"Lecture 15: Optimal Control and Planning"}),(0,d.jsx)(t.td,{children:"HW3 Due; HW4 Out"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/18"}),(0,d.jsx)(t.td,{children:"Lecture 16: Model-Based RL Part 1"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/20"}),(0,d.jsx)(t.td,{children:"Lecture 17: Model-Based RL Part 2"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/25"}),(0,d.jsx)(t.td,{children:"Lecture 18: Guest Lecture (Yunzhu Li)"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"3/27"}),(0,d.jsx)(t.td,{children:"Lecture 19: Offline RL and Inverse RL Basics"}),(0,d.jsx)(t.td,{children:"Proposal Due"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/01"}),(0,d.jsx)(t.td,{children:"Lecture 20: Guest Lecture: Advanced Offline RL"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/03"}),(0,d.jsx)(t.td,{children:"Lecture 21: Safe RL and Safe Robot Learning"}),(0,d.jsx)(t.td,{children:"HW4 Due"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/08"}),(0,d.jsx)(t.td,{children:"Lecture 22: Robot Simulation and Sim2Real"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/10"}),(0,d.jsx)(t.td,{children:"Lecture 23: Multi-Task/Adaptive Robot Learning"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/15"}),(0,d.jsx)(t.td,{children:"Lecture 24: Guest Lecture: LLM + Robotics"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/17"}),(0,d.jsx)(t.td,{children:"Lecture 25: Student Project Presentations"}),(0,d.jsx)(t.td,{children:"Presentation Due"})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/22"}),(0,d.jsx)(t.td,{children:"Lecture 26: Student Project Presentations"}),(0,d.jsx)(t.td,{})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:"4/24"}),(0,d.jsx)(t.td,{children:"Lecture 27: Student Project Poster Session"}),(0,d.jsx)(t.td,{})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,d.jsx)(t,{...e,children:(0,d.jsx)(j,{...e})}):j(e)}},1151:(e,t,r)=>{r.d(t,{Z:()=>i,a:()=>c});var d=r(7294);const s={},n=d.createContext(s);function c(e){const t=d.useContext(n);return d.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),d.createElement(n.Provider,{value:t},e.children)}}}]);